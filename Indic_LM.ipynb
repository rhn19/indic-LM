{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Indic-LM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2r2vwD-i12P"
      },
      "source": [
        "# Indo-Iranian Languages (North India) - Hindi, Urdu, Punjabi, Gujarati, Bengali, Marathi, Odia, Nepali\n",
        "# https://en.m.wikipedia.org/wiki/Indo-Iranian_languages\n",
        "# Dravidian Languages (South India) - Tamil, Telugu, Malayalam, Kannada\n",
        "# https://en.m.wikipedia.org/wiki/Dravidian_languages\n",
        "# Sanksrit - Mixture of the two"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9w28_HFZvq_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puvyIZpFZ_ZE"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMena-w8Wwnu"
      },
      "source": [
        "#Install & Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FozzvKwnZ_-8"
      },
      "source": [
        "#Install older versions to avoid migration issues\n",
        "#sentencepiece-0.1.91 \n",
        "#tokenizers-0.8.1\n",
        "#transformers-3.0.2\n",
        "\n",
        "!pip install sentencepiece==0.1.91 tokenizers==0.8.1\n",
        "!git clone --depth 1 -b v3.0.2 https://github.com/huggingface/transformers.git\n",
        "!pip install transformers/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YnlyeMMaXp7"
      },
      "source": [
        "import sentencepiece as spm\n",
        "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGn6VzOgXeE8"
      },
      "source": [
        "# Set the language (2-letter code)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cPtFfx0b5MP"
      },
      "source": [
        "lang = \"mr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAzrjLcLqDt2"
      },
      "source": [
        "# Preparing Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9FFelgQqDN6"
      },
      "source": [
        "!wget \"https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/data/monolingual/indicnlp_v1/sentence/mr.txt.gz\"\n",
        "!gunzip mr.txt.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tVLaijDqhuB"
      },
      "source": [
        "lines = []\n",
        "with open(f\"/content/{lang}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        lines.append(line)\n",
        "\n",
        "# print(len(lines))\n",
        "# print(lines[:5])\n",
        "# print(lines[-5:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Snkg3saf_dd"
      },
      "source": [
        "#For kn, or, pa\n",
        "with open(f\"{lang}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(lines[1:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wGDnhJrq5FA"
      },
      "source": [
        "train, test = train_test_split(lines, shuffle=True, test_size=0.1, random_state=19)\n",
        "print(len(train), len(test))\n",
        "\n",
        "with open(f\"{lang}_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(train)\n",
        "\n",
        "with open(f\"{lang}_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.writelines(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h3D6xCRs3AP"
      },
      "source": [
        "!rm -rf mr.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxLnl763rD5S"
      },
      "source": [
        "!cp /content/mr_test.txt /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/\n",
        "!cp /content/mr_train.txt /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2WwgRvXoZ3I"
      },
      "source": [
        "#Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2s24_ktihRX"
      },
      "source": [
        "Before you load data, you would need the following directory structure:<br>\n",
        "Marathi-LM<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;>data<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;>(The dataset you would be using)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;>model_config<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;>tokenizers (Link provided on the github readme)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;>tokenizer_config.json (Available on the github repo)<br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;>config.json (Available on the github repo)<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWAH4Z7zoZVD"
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/data/wiki-latest/mr_clean.txt ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJKMwCCG8Jhu"
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/model_config/ .\n",
        "!rm -rf /content/model_config/tokenizers\n",
        "# !cp /content/mr_0.9995_unigram_32000_spiece.model /content/model_config/spiece.model\n",
        "# !cp /content/mr_0.9995_unigram_32000_spiece.vocab /content/model_config/spiece.vocab\n",
        "!cp /content/mr_2_bpe_32000-merges.txt /content/model_config/merges.txt\n",
        "!cp /content/mr_2_bpe_32000-vocab.json /content/model_config/vocab.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESWfzLWDCoa-"
      },
      "source": [
        "!cp -r /content/model_config /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/data/wiki-latest/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3x8l6BcaK5S"
      },
      "source": [
        "# Training Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJVCyXFuaJ2k"
      },
      "source": [
        "#SENTENCEPIECE\n",
        "vocab_size = 32000\n",
        "char_cov = 0.9995\n",
        "model_type = \"unigram\"\n",
        "\n",
        "# Try --input_sentence_size=10000000 (or smaller) which allows to sample sentences before training.\n",
        "# Generally speaking, 1M-10M sentences are enough for training reasonably good model.\n",
        "\n",
        "spm.SentencePieceTrainer.Train(f'--input=/content/{lang}.txt \\\n",
        "                                --model_prefix={lang}_{char_cov}_{model_type}_{vocab_size}_spiece \\\n",
        "                                --vocab_size={vocab_size} \\\n",
        "                                --character_coverage={char_cov} \\\n",
        "                                --model_type={model_type} \\\n",
        "                                --control_symbols=[CLS],[SEP],[MASK] \\\n",
        "                                --shuffle_input_sentence=True')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z34NtdqVadqU"
      },
      "source": [
        "#WORDPIECE\n",
        "vocab_size = 32000\n",
        "min_frequency = 2\n",
        "limit_alphabet = 1000\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False,\n",
        "    strip_accents=False,\n",
        ")\n",
        "\n",
        "tokenizer.train(f\"/content/{lang}.txt\", \n",
        "                vocab_size=vocab_size, \n",
        "                min_frequency=min_frequency, \n",
        "                show_progress=True, \n",
        "                special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'], \n",
        "                limit_alphabet=limit_alphabet, \n",
        "                wordpieces_prefix=\"##\")\n",
        "\n",
        "tokenizer.save_model(\"/content/\", f\"{lang}_{min_frequency}_wordpiece_{vocab_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjrJruRVbJiw"
      },
      "source": [
        "#BPE\n",
        "vocab_size = 32000\n",
        "min_frequency = 2\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "tokenizer.train(f\"/content/{lang}.txt\", \n",
        "                vocab_size=vocab_size, \n",
        "                min_frequency=min_frequency, \n",
        "                show_progress=True, \n",
        "                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "\n",
        "tokenizer.save_model(\"/content/\", f\"{lang}_{min_frequency}_bpe_{vocab_size}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaCEm_AhoiQX"
      },
      "source": [
        "# Training Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFoWgLqpogN6"
      },
      "source": [
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eBxAmcdoo8K"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDSZb769ortI"
      },
      "source": [
        "#TRAIN FROM SCRATCH\n",
        "!python /content/transformers/examples/language-modeling/run_language_modeling.py \\\n",
        "        --model_type roberta \\\n",
        "        --config_name /content/model_config/ \\\n",
        "        --tokenizer_name /content/model_config/ \\\n",
        "        --train_data_file /content/mr_train.txt \\\n",
        "        --eval_data_file /content/mr_test.txt \\\n",
        "        --output_dir /content/outputs \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --mlm \\\n",
        "        --learning_rate 1e-4 \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 2500 \\\n",
        "        --logging_steps 2500 \\\n",
        "        --save_total_limit 10 \\\n",
        "        --num_train_epochs 20 \\\n",
        "        --per_device_eval_batch_size 32 \\\n",
        "        --per_device_train_batch_size 32 \\\n",
        "        --block_size 256 \\\n",
        "        --logging_dir logs \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n-3Qx_doxQY"
      },
      "source": [
        "#TRAIN FROM CHECKPOINT\n",
        "!python /content/transformers/examples/language-modeling/run_language_modeling.py \\\n",
        "        --model_name_or_path /content/outputs/checkpoint-450000 \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/outputs/checkpoint-450000/ \\\n",
        "        --tokenizer_name /content/model_config/ \\\n",
        "        --train_data_file /content/data/indic-nlp/mr_train.txt \\\n",
        "        --eval_data_file /content/data/indic-nlp/mr_dev.txt \\\n",
        "        --output_dir /content/outputs \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --mlm \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 2500 \\\n",
        "        --logging_steps 2500 \\\n",
        "        --save_total_limit 10 \\\n",
        "        --num_train_epochs 20 \\\n",
        "        --per_device_eval_batch_size 32 \\\n",
        "        --per_device_train_batch_size 32 \\\n",
        "        --block_size 256 \\\n",
        "        --logging_dir logs \\\n",
        "        --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu66pUdLpTaI"
      },
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYuRR86opRbp"
      },
      "source": [
        "!python /content/transformers/examples/language-modeling/run_language_modeling.py \\\n",
        "        --model_name_or_path /content/outputs/checkpoint-550000 \\\n",
        "        --model_type albert-base-v2 \\\n",
        "        --config_name /content/outputs/checkpoint-550000/ \\\n",
        "        --tokenizer_name /content/model_config/ \\\n",
        "        --eval_data_file /content/data/indic-nlp/mr_dev.txt \\\n",
        "        --output_dir /content/outputs \\\n",
        "        --do_eval \\\n",
        "        --mlm \\\n",
        "        --line_by_line \\\n",
        "        --save_steps 2500 \\\n",
        "        --logging_steps 2500 \\\n",
        "        --save_total_limit 10 \\\n",
        "        --num_train_epochs 20 \\\n",
        "        --per_device_eval_batch_size 32 \\\n",
        "        --per_device_train_batch_size 32 \\\n",
        "        --block_size 256 \\\n",
        "        --logging_dir logs \\\n",
        "        --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv2qRNcxfMqa"
      },
      "source": [
        "# Save Models & Logs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRHTD1Zqo5lh"
      },
      "source": [
        "!mkdir /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/outputs\n",
        "!mkdir /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/logs\n",
        "!cp -r /content/outputs/* /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/outputs/\n",
        "!cp -r /content/logs /content/drive/My\\ Drive/Colab\\ Notebooks/Marathi-LM/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}